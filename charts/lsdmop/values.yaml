# Elastic Operator
# To get a latest values you can run:
# helm show values elastic/eck-operator
eck-operator:
  # installCRDs must be disabled because CRDs are put into the Helm chart
  installCRDs: false
  managedNamespaces: ["lsdmop"]
  createClusterScopedResources: false
  replicaCount: 1
  resources:
    limits:
      cpu: 1
      memory: 1Gi
    requests:
      cpu: 100m
      memory: 150Mi
  # tracing:
  #   # enabled specifies whether APM tracing is enabled for the operator.
  #   enabled: false
  #   # config is a map of APM Server configuration variables that should be set in the environment.
  #   config:
  #     ELASTIC_APM_SERVER_URL: http://localhost:8200
  #     ELASTIC_APM_SERVER_TIMEOUT: 30s
  config:
    setDefaultSecurityContext: "auto-detect"
    validateStorageClass: false
  webhook:
    enabled: false    

# LSDobserve
lsdmop:
  # There are your options clusterType: openshift | gke | rancher
  clusterType: "rancher"
  eckVersion: &anchoreckVersion "8.4.1"
  eckElasticsearchImage: &anchoreckElasticsearchImage "docker.elastic.co/elasticsearch/elasticsearch:8.4.1"
  eckKibanaImage: &anchoreckKibanaImage "docker.elastic.co/kibana/kibana:8.4.1"
  eckApmImage: &anchoreckApmImage "docker.elastic.co/apm/apm-server:8.4.1"
  eckFilebeatImage: &anchoreckFilebeatImage "docker.elastic.co/beats/filebeat:8.4.1"
  eckMetricbeatImage: &anchoreckMetricbeatImage "docker.elastic.co/beats/metricbeat:8.4.1"
  kibanaURL: &anchorkibanaURL "kibana.domain.example"
  elasticURL: &anchorelasticURL "elastic.domain.example"
  apmURL: &anchorapmURL "apm.domain.example"
  fleetURL: &anchorfleetURL "fleet.domain.example"
  logstashJavaOpts: &anchorlogstashJavaOpts "-Xms1g -Xmx1g"
  grafanaUsername: &anchorgrafanaUsername "admin"
  grafanaPassword: &anchorgrafanaPassword "ChangeMe-PasswordForGrafana"
  grafanaVersion: &anchorgrafanaVersion "7.5.2"
  grafanaURL: &anchorgrafanaURL "grafana.domain.example"
  prometheusRenention: &anchorprometheusRenention "30d"
  prometheusURL: &anchorprometheusURL "prometheus.domain.example"
  prometheusHttpURL: &anchorprometheusHttpURL "https://prometheus.domain.example"
  prometheusStorageSize: &anchorprometheusStorageSize "25Gi"
  alertmanagerURL: &anchoralertmanagerURL "alertmanager.domain.example"
  alertmanagerHttpURL: &anchoralertmanagerHttpURL "https://alertmanager.domain.example"
  smtpHost: &anchorsmtpHost "smtp.lsdopen.io"
  smtpPort: &anchorsmtpPort "25"
  smtpSmartHost: &anchorsmtpSmartHost "smtp.lsdopen.io:25"
  supportAddress: &anchorsupportAddress "address-that-must-receive-alerts@lsdopen.io"
  fromAddress: &anchorfromAddress "lsdmop+noreply+infra-01.qa.lsdopen.io@lsdopen.io"
  fromName: &anchorfromName "LSDobserve - LSD - k8s-01.qa.lsdopen.io@lsdopen.io"
  storageClass: &anchorstorageClass "ebs-gp3"
  #storageClass: &anchorstorageClass "local"
  curl:
    image: docker.io/curlimages/curl:7.74.0
  grafana:
    enabled: false
    ingress:
      url: *anchorgrafanaURL
  elastic:
    enabled: true
    license:
      enabled: false
      key: ""
    autoscaling:
      enabled: false
    image: *anchoreckElasticsearchImage
    version: *anchoreckVersion
    ingress:
      annotations: {}
      tls:
        secretName: elastic-ingress-tls
      url: *anchorelasticURL
    serviceAccount:
      annotations: {}    
    cluster:
      hot:
        count: 3
        maxcount: 3 #autoscaling.enabled must be true otherwise ignored
        mem: 30
        cpureq: 500m
        cpulim: 4000m
        tolerations: elastic-hot
        storagesize: 560Gi
        storageclass: ebs-gp3
        nodeSelector: null
      warm:
        count: 2
        maxcount: 2 #autoscaling.enabled must be true otherwise ignored        
        mem: 30
        cpureq: 500m
        cpulim: 4000m
        tolerations: elastic-warm
        storagesize: 2760Gi
        storageclass: ebs-gp3
        nodeSelector: null
      # cold:
      #   count: 0
      #   mem: 30
      #   cpureq: 500m
      #   cpulim: 4000m
      #   tolerations: elastic-cold
      #   storagesize: 560Gi
      #   storageclass: ebs-gp3
      # frozen:
      #   count: 0
      #   mem: 30
      #   cpureq: 500m
      #   cpulim: 4000m
      #   tolerations: elastic-frozen
      #   storagesize: 560Gi
      #   storageclass: ebs-gp3
      ml:
        count: 2
        maxcount: 2 #autoscaling.enabled must be true otherwise ignored        
        mem: 16
        cpureq: 500m
        cpulim: 4000m
        storagesize: 1Gi
        storageclass: ebs-gp3

    # Active Directory Integration

    ad:
      enabled: false
      # host: ldap://domain.example.com:389
      # basedn: DC=example,DC=com
      # userdn: cn=User,ou=example,o=com
      # admindn: cn=Admin,ou=example,o=com
      # binddn: CN=UserName,OU=Support,DC=example,DC=com
      # bindpassword: supersecurepassword
      # adhostname: example
      # adfqdn: example.com
      # adip: 123.123.123.123
      # adddz: domaindnszones.example.com
      # adfdz: forestdnszones.example.com

    #Kibana
    kibana:
      enabled: true
      image: *anchoreckKibanaImage
      version: *anchoreckVersion
      count: "2"
      ingress:
        url: *anchorkibanaURL
        enabled: true
        className: nginx
        annotations: {}
        tls:
          secretName: kibana-ingress-tls

    #Filebeat 
    filebeat:
      enabled: true
      image: *anchoreckFilebeatImage
      version: *anchoreckVersion
      serviceAccount:
        annotations: {}

    #Cloudwatch Input Source for Filebeat
    filebeatCloudwatch:
      enabled: false
      roleArn: ""
      logsArn: ""


    ## Metricbeat as a Daemonset to collect metrics from Nodes
    metricbeat:
      enabled: true
      image: *anchoreckMetricbeatImage
      version: *anchoreckVersion
    ## Fleet agents as a Daemonset to collect metrics/logs from Nodes (replaces metricbeat and filebeat)
    fleet:
      enabled: true
      version: *anchoreckVersion
      fleetURL: *anchorfleetURL
      ingress:
        annotations: {}
        tls:
          secretName: fleet-ingress-tls
        url: *anchorfleetURL

    ## Metricbeat to connect to Prometheus
    metricbeatPrometheus:
      enabled: false
      image: *anchoreckMetricbeatImage
      version: *anchoreckVersion

    #APM servers  
    apm:
      enabled: false
      image: *anchoreckApmImage
      version: *anchoreckVersion
      count: 2
      ingress:
        annotations: {}
        tls:
          secretName: apm-ingress-tls
        url: *anchorapmURL

    #New gen Fleet based APM servers - cannot be enabled if apm is enabled
    fleetapm:
      enabled: true
      image: *anchoreckApmImage
      version: *anchoreckVersion
      ingress:
        annotations: {}
        tls:
          secretName: apm-ingress-tls
        url: *anchorapmURL


    #S3 bucket storage for snapshots    
    s3snapshot:
      enabled: true
      bucket: "elastic-storage"


# Logstash
logstash:
  image: "docker.elastic.co/logstash/logstash"
  imageTag: *anchoreckVersion
  replicas: "0"
  logstashJavaOpts: *anchorlogstashJavaOpts
  resources:
    requests:
      cpu: "100m"
      memory: "1536Mi"
    limits:
      cpu: "1000m"
      memory: "1536Mi"
  volumeClaimTemplate: {}
  service: 
    type: ClusterIP
    loadBalancerIP: ""
    ports:
      - name: beats
        port: 5044
        protocol: TCP
        targetPort: 5044
  logstashPatternDir: "/usr/share/logstash/patterns/"
  logstashConfig: 
    logstash.yml: |
      http.host: "0.0.0.0"
      monitoring.enabled: true
      monitoring.elasticsearch.username: elastic
      monitoring.elasticsearch.password: ${ELASTIC_PASSWORD}
      monitoring.elasticsearch.hosts: [ "https://lsdmop-es-http:9200" ]
      monitoring.elasticsearch.ssl.certificate_authority: /usr/share/logstash/certs/tls.crt
      pipeline.ordered: false
    pipelines.yml: |
      - pipeline.id: main
        path.config: "/usr/share/logstash/pipeline"
      - pipeline.id: lsdmop
        path.config: "/usr/share/logstash/pipeline/lsdmop"
  secretMounts:
    - name: elastic-ca-certs
      secretName: lsdmop-es-http-ca-internal
      path: /usr/share/logstash/certs
      defaultMode: 420
  extraEnvs:
    - name: ELASTIC_PASSWORD
      valueFrom:
        secretKeyRef:
          name: lsdmop-es-elastic-user
          key: elastic
  extraVolumes: |
    - configMap:
        defaultMode: 420
        name: logstash-pipeline
      name: logstash-pipeline
    - configMap:
        defaultMode: 420
        name: logstash-pipeline-lsdmop
      name: logstash-pipeline-lsdmop
    - configMap:
        defaultMode: 420
        name: logstash-patterns
      name: logstash-patterns
  extraVolumeMounts: |
    - mountPath: /usr/share/logstash/pipeline
      name: logstash-pipeline
    - mountPath: /usr/share/logstash/pipeline/lsdmop
      name: logstash-pipeline-lsdmop
    - mountPath: /usr/share/logstash/patterns
      name: logstash-patterns






  















# Grafana
# To get a latest values you can run:
# helm show values grafana/grafana

grafana:
  adminUser: *anchorgrafanaUsername
  adminPassword: *anchorgrafanaPassword
  image:
    repository: grafana/grafana
    tag: *anchorgrafanaVersion
  # If you want to install on specific nodes, for example Infra nodes on Openshift
  #nodeSelector:
    #node-role.kubernetes.io/worker: "true"
    #kubernetes.io/os: linux
  replicas: 1
  deploymentStrategy: { "type": "Recreate" }
  # ingress is disable because it is created via the lsd-observe template
  ingress:
    enabled: false
  resources:
    limits:
      cpu: 100m
      memory: 128Mi
    requests:
      cpu: 10m
      memory: 128Mi
  persistence:
    enabled: "true"
    type: pvc
    size: 1Gi
    storageClassName: *anchorstorageClass
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        - name: LSDobserve - Prometheus
          type: prometheus
          url: http://lsdmop-prometheus-server
          isDefault: true
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
        - name: "ds-01"
          orgId: 1
          folder: "LSDcontainer"
          type: file
          disableDeletion: false
          editable: false
          options:
            path: /var/lib/grafana/dashboards/ds-01
  dashboardsConfigMaps:
    ds-01: "grafana-dashboard-kubernetes-overview"
    ds-02: "grafana-dashboard-namespace-details"
    ds-03: "grafana-dashboard-node-namespace-details"
    ds-04: "grafana-dashboard-rook-ceph-overview"
    ds-05: "node-exporter-for-prometheus-dashboard"
  imageRenderer:
    enabled: false
    replicas: 1
    image:
      repository: grafana/grafana-image-renderer
      tag: latest
    service:
      portName: "http"
      port: 8081
    podPortName: http
    revisionHistoryLimit: 10
    networkPolicy:
      limitIngress: true
      limitEgress: false
    resources:
      limits:
        cpu: 100m
        memory: 100Mi
      requests:
        cpu: 50m
        memory: 50Mi














# Prometheus
# To get a latest values you can run:
# helm show values prometheus-community/prometheus

prometheus:
  configmapReload:
    prometheus:
      enabled: true
      name: configmap-reload
      image:
        repository: jimmidyson/configmap-reload
        tag: v0.4.0
    alertmanager:
      enabled: true
      name: configmap-reload
      image:
        repository: jimmidyson/configmap-reload
        tag: v0.4.0
  # https://kubernetes.github.io/kube-state-metrics
  kubeStateMetrics:
    enabled: true
    image:
      repository: quay.io/coreos/kube-state-metrics
      tag: v1.9.7
  server:
    image:
      repository: quay.io/prometheus/prometheus
      tag: v2.26.0
    replicaCount: 0
    baseURL: *anchorprometheusHttpURL
    retention: *anchorprometheusRenention
    statefulSet:
      enabled: true
    ingress:
      enabled: true
      hosts:
        - *anchorprometheusURL
    persistentVolume:
      enabled: true
      accessModes:
        - ReadWriteOnce
      size: *anchorprometheusStorageSize
    resources:
      limits:
        cpu: 500m
        memory: 2Gi
      requests:
        cpu: 100m
        memory: 512Mi
  pushgateway:
    enabled: false
  nodeExporter:
    image:
      repository: quay.io/prometheus/node-exporter
      tag: v1.0.1
    enabled: false
    hostNetwork: true
    hostPID: true
    name: node-exporter
    pod:
      labels:
        org: lsd
        product: lsdmop
        name: node-exporter
    resources:
      limits:
        cpu: 200m
        memory: 50Mi
      requests:
        cpu: 10m
        memory: 10Mi
    securityContext: {}
    service:
      annotations:
        prometheus.io/scrape: "true"
      labels: {}
      clusterIP: None
      externalIPs: []
      hostPort: 8100
      loadBalancerIP: ""
      loadBalancerSourceRanges: []
      servicePort: 8100
      type: ClusterIP
    tolerations:
      - key: "node-role.kubernetes.io/controlplane"
        operator: "Exists"
      - key: "node-role.kubernetes.io/etcd"
        operator: "Exists"
      - key: "node-role.kubernetes.io/master"
        operator: "Exists"
  alertmanager:
    enabled: true
    replicaCount: 1
    prefixURL: ""
    baseURL: *anchoralertmanagerHttpURL
    service:
      enableMeshPeer: true
    statefulSet:
      enabled: true
      annotations: {}
      labels: {}
      podManagementPolicy: OrderedReady
      headless:
        annotations: {}
        labels: {}
        enableMeshPeer: true
    image:
      repository: prom/alertmanager
      tag: v0.21.0
    ingress:
      enabled: true
      hosts:
        - *anchoralertmanagerURL
    resources:
      limits:
        cpu: 100m
        memory: 128Mi
      requests:
        cpu: 10m
        memory: 32Mi
    persistentVolume:
      enabled: true
      accessModes:
        - ReadWriteOnce
      size: 1Gi
  alertmanagerFiles:
    alertmanager.yml:
      global:
        resolve_timeout: 5m
      receivers:
        - name: LSD Support
          email_configs:
            - to: *anchorsupportAddress
              from: *anchorfromAddress
              smarthost: *anchorsmtpSmartHost
              require_tls: false
              auth_username: ""
              auth_password: ""
      route:
        group_by:
          - job
        group_interval: 5m
        group_wait: 30s
        receiver: "LSD Support"
        repeat_interval: 12h
        routes:
          - receiver: LSD Support
            match:
              severity: warning
          - receiver: LSD Support
            match:
              severity: critical
  serverFiles:
    alerting_rules.yml:
      groups:
        - name: Instances
          rules:
            - alert: InstanceDown
              expr: up == 0
              for: 5m
              labels:
                severity: warning
              annotations:
                description: "{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes."
                message: "When Instance/Endpoints are marked as down it means Prometheus cannot scrape those targets. You can get more details by gong to the Prometheus frontend, going to Status and Targets. You always want all you Instance/Endpoints up else you will not be able to monitor anything"
        - name: NodesMarkedAsUnscheduled
          rules:
            - alert: NodesMarkedAsUnscheduled
              expr: kube_node_spec_unschedulable > 0
              for: 1h
              labels:
                severity: warning
              annotations:
                description: "{{ $labels.kubernetes_node }} is marked as Unscheduled for longer than 1h minutes."
                message: "When Nodes are marked Unscheduled no new pods will be scheduled "
        - name: NodeMemoryUsageAbove85Percent
          rules:
            - alert: NodeMemoryUsageAbove85Percent
              expr: 100 * (1 - ((avg_over_time(node_memory_MemFree_bytes[2m]) + avg_over_time(node_memory_Cached_bytes[2m]) + avg_over_time(node_memory_Buffers_bytes[2m]) + avg_over_time(node_memory_SReclaimable_bytes[2m])) / avg_over_time(node_memory_MemTotal_bytes[2m]))) > 85 < 90
              for: 1h
              labels:
                severity: warning
              annotations:
                description: '{{ $labels.kubernetes_node }} is using {{ printf "%.0f" $value }}% of the total memory'
                message: "When the memory of a node is exhausted pods will be evicted and sacrificed to keep the node ready. It is recommended that you cordon node {{ $labels.kubernetes_node }} and delete a couple of pods on the node, forcing them to start up on another node, then uncordone {{ $labels.kubernetes_node }}. You can find more info here: https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/"
        - name: NodeMemoryUsageAbove95Percent
          rules:
            - alert: NodeMemoryUsageAbove95Percent
              expr: 100 * (1 - ((avg_over_time(node_memory_MemFree_bytes[2m]) + avg_over_time(node_memory_Cached_bytes[2m]) + avg_over_time(node_memory_Buffers_bytes[2m]) + avg_over_time(node_memory_SReclaimable_bytes[2m])) / avg_over_time(node_memory_MemTotal_bytes[2m]))) > 95
              for: 1h
              labels:
                severity: critical
              annotations:
                description: '{{ $labels.kubernetes_node }} is using {{ printf "%.0f" $value }}% of the total memory'
                message: "When the memory of a node is exhausted pods will be evicted and sacrificed to keep the node ready. It is recommended that you cordon node {{ $labels.kubernetes_node }} and delete a couple of pods on the node, forcing them to start up on another node, then uncordone {{ $labels.kubernetes_node }}. You can find more info here: https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/"
        - name: PVCUsageOver85Percent
          rules:
            - alert: PVCUsageOver85Percent
              expr: (kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes) * 100 > 85
              for: 1h
              labels:
                severity: warning
              annotations:
                description: 'PVC {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }} is using {{ printf "%.0f" $value }}% of the capacity'
                message: 'You will need to go into the Pod that is using that PVC and clean up some storage. For example "kubectl -n {{ $labels.namespace }} exec -it PODNAME -- sh"'
        - name: PVCUsageOver95Percent
          rules:
            - alert: PVCUsageOver95Percent
              expr: (kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes) * 100 > 95
              for: 1h
              labels:
                severity: critical
              annotations:
                description: 'PVC {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }} is using {{ printf "%.0f" $value }}% of the capacity'
                message: 'You will need to go into the Pod that is using that PVC and clean up some storage. For example "kubectl -n {{ $labels.namespace }} exec -it PODNAME -- sh"'
        - name: NodeFileSystemUsageOver85Percent
          rules:
            - alert: NodeFileSystemUsageOver85Percent
              expr: 100 - ((node_filesystem_avail_bytes {mountpoint!~".*/host/.*",mountpoint!~".*/etc/.*",mountpoint!~".*/run/secrets.*",mountpoint!~".*/var/run/.*"} * 100) / node_filesystem_size_bytes {mountpoint!~".*/host/.*",mountpoint!~".*/etc/.*",mountpoint!~".*/run/secrets.*",mountpoint!~".*/var/run/.*"}) > 85
              for: 1h
              labels:
                severity: warning
              annotations:
                description: 'Mount point {{ $labels.mountpoint }} on Node {{ $labels.kubernetes_node }} is at {{ printf "%.0f" $value }}% of total capacity'
                message: "You will need to SSH into Node {{ $labels.kubernetes_node }} and clean up storage on {{ $labels.mountpoint }}"
        - name: NodeFileSystemUsageOver95Percent
          rules:
            - alert: NodeFileSystemUsageOver95Percent
              expr: 100 - ((node_filesystem_avail_bytes {mountpoint!~".*/host/.*",mountpoint!~".*/etc/.*",mountpoint!~".*/run/secrets.*",mountpoint!~".*/var/run/.*"} * 100) / node_filesystem_size_bytes {mountpoint!~".*/host/.*",mountpoint!~".*/etc/.*",mountpoint!~".*/run/secrets.*",mountpoint!~".*/var/run/.*"}) > 95
              for: 1h
              labels:
                severity: critical
              annotations:
                description: 'Mount point {{ $labels.mountpoint }} on Node {{ $labels.kubernetes_node }} is at {{ printf "%.0f" $value }}% of total capacity'
                message: "You will need to SSH into Node {{ $labels.kubernetes_node }} and clean up storage on {{ $labels.mountpoint }}"
        - name: TotalAvailableCPURequestsOver90Percent
          rules:
            - alert: TotalAvailableCPURequestsOver90Percent
              expr: (sum ((sum(kube_pod_container_resource_requests_cpu_cores{container!="deployment",container!="docker-build",namespace!="logging",namespace!="default",namespace!~".*openshift-.*",namespace!~".*openmonitoring.*",namespace!~".*kube-.*"} > 0) by (container,pod) / count(kube_pod_container_status_running > 0) by (container,pod))*1000)) / (sum (kube_node_status_allocatable_cpu_cores)*1000) * 100 > 90
              for: 1h
              labels:
                severity: critical
              annotations:
                description: 'The total allowed CPU requests are at {{ printf "%.0f" $value }}%'
                message: "When the total allowed CPU requests hits 100% no more pods will be allowed to start up. You either need to lower the CPU requests of pods or add more CPU into the cluster"
        - name: NodeCpuUtilizationOver95Percent
          rules:
            - alert: NodeCpuUtilizationOver95Percent
              expr: 100 - (avg by (kubernetes_node) (rate( node_cpu_seconds_total {mode="idle"}[2m])) * 100) > 95
              for: 1h
              labels:
                severity: warning
              annotations:
                description: 'Node {{ $labels.kubernetes_node }} has a CPU utlization over 95% for over 1 hour. Current CPU utilization of Node {{ $labels.kubernetes_node }} is {{ printf "%.0f" $value }}%'
                message: "When the CPU is maxed out for over an hour it indicates an that a process is killing the node or the node does not have enough CPU assigned to it"
        - name: NodeLoadOver50
          rules:
            - alert: NodeLoadOver50
              expr: node_load15 > 50
              for: 1h
              labels:
                severity: critical
              annotations:
                description: 'Node {{ $labels.kubernetes_node }} has a 15 minute load average of {{ printf "%.0f" $value }}%'
                message: "When the load is over 50 this indicates an issue of high CPU, low memory (going into swap) and high disk utlization"
        - name: RookCephClusterOver90Percent
          rules:
            - alert: RookCephClusterOver90Percent
              expr: 100 - (sum without(instance) ((ceph_cluster_total_bytes - ceph_cluster_total_used_bytes) / ceph_cluster_total_bytes) * 100) > 90
              for: 5m
              labels:
                severity: critical
              annotations:
                description: 'Namespace {{ $labels.kubernetes_namespace }} contains a Ceph cluster with a current usage of {{ printf "%.0f" $value }}%'
                message: "Ceph clusters over 95% can begin to fail and need to be expanded or pruned"
        - name: PodNotReady
          rules:
            - alert: PodNotReady
              expr: sum by(namespace, pod, container) (kube_pod_container_status_waiting_reason{namespace=~"(openshift-.*|kube-.*|default|logging)"}) > 0
              for: 30m
              labels:
                severity: warning
              annotations:
                description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready state for longer than 30 minutes"
                message: "Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready state for longer than 30 minutes"




